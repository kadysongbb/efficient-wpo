{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "import csv\n",
    "import gym_gridworld\n",
    "import itertools\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 return -52.0000003 discounted reward -9.9582543\n",
      "Episode 1 return -52.0000003 discounted reward -9.9582543\n",
      "Episode 2 return 65.0000003 discounted reward -7.2465293\n",
      "Episode 3 return -52.0000003 discounted reward -9.9582543\n",
      "Episode 4 return -52.0000003 discounted reward -9.9582543\n",
      "Episode 5 return -52.0000003 discounted reward -9.9582543\n",
      "Episode 6 return 51.0000003 discounted reward -9.3700943\n",
      "Episode 7 return -52.0000003 discounted reward -9.9582543\n",
      "Episode 8 return -52.0000003 discounted reward -9.9582543\n",
      "Episode 9 return -52.0000003 discounted reward -9.9582543\n",
      "Episode 10 return -52.0000003 discounted reward -9.9582543\n",
      "Episode 11 return -52.0000003 discounted reward -9.9582543\n",
      "Episode 12 return 56.0000003 discounted reward -8.9332493\n",
      "Episode 13 return -52.0000003 discounted reward -9.9582543\n",
      "Episode 14 return -52.0000003 discounted reward -9.9582543\n",
      "Episode 15 return 57.0000003 discounted reward -8.8147213\n",
      "Episode 16 return 53.0000003 discounted reward -9.2223383\n",
      "Episode 17 return -52.0000003 discounted reward -9.9582543\n",
      "Episode 18 return 95.0000003 discounted reward 54.9539003\n",
      "Episode 19 return 92.0000003 discounted reward 37.3513933\n",
      "Episode 20 return 87.0000003 discounted reward 17.9605243\n",
      "Episode 21 return 76.0000003 discounted reward -1.2256913\n",
      "Episode 22 return 85.0000003 discounted reward 12.6480253\n",
      "Episode 23 return 84.0000003 discounted reward 10.3832223\n",
      "Episode 24 return 94.0000003 discounted reward 48.4585103\n",
      "Episode 25 return 89.0000003 discounted reward 24.5191663\n",
      "Episode 26 return 90.0000003 discounted reward 28.3546283\n",
      "Episode 27 return 91.0000003 discounted reward 32.6162543\n",
      "Episode 28 return 84.0000003 discounted reward 10.3832223\n",
      "Episode 29 return -52.0000003 discounted reward -9.9582543\n",
      "Episode 30 return 79.0000003 discounted reward 2.0360893\n",
      "Episode 31 return 84.0000003 discounted reward 10.3832223\n",
      "Episode 32 return 75.0000003 discounted reward -2.1031223\n",
      "Episode 33 return 82.0000003 discounted reward 6.5104103\n",
      "Episode 34 return 80.0000003 discounted reward 3.3734323\n",
      "Episode 35 return 88.0000003 discounted reward 21.0672493\n",
      "Episode 36 return 93.0000003 discounted reward 42.6126593\n",
      "Episode 37 return 87.0000003 discounted reward 17.9605243\n",
      "Episode 38 return 90.0000003 discounted reward 28.3546283\n",
      "Episode 39 return 94.0000003 discounted reward 48.4585103\n",
      "Episode 40 return 80.0000003 discounted reward 3.3734323\n",
      "Episode 41 return 87.0000003 discounted reward 17.9605243\n",
      "Episode 42 return 91.0000003 discounted reward 32.6162543\n",
      "Episode 43 return 80.0000003 discounted reward 3.3734323\n",
      "Episode 44 return 90.0000003 discounted reward 28.3546283\n",
      "Episode 45 return 81.0000003 discounted reward 4.8593693\n",
      "Episode 46 return 80.0000003 discounted reward 3.3734323\n",
      "Episode 47 return 88.0000003 discounted reward 21.0672493\n",
      "Episode 48 return 94.0000003 discounted reward 48.4585103\n",
      "Episode 49 return 88.0000003 discounted reward 21.0672493\n",
      "Episode 50 return 92.0000003 discounted reward 37.3513933\n",
      "Episode 51 return 88.0000003 discounted reward 21.0672493\n",
      "Episode 52 return 82.0000003 discounted reward 6.5104103\n",
      "Episode 53 return 93.0000003 discounted reward 42.6126593\n",
      "Episode 54 return 89.0000003 discounted reward 24.5191663\n",
      "Episode 55 return 92.0000003 discounted reward 37.3513933\n",
      "Episode 56 return 93.0000003 discounted reward 42.6126593\n",
      "Episode 57 return 93.0000003 discounted reward 42.6126593\n",
      "Episode 58 return 90.0000003 discounted reward 28.3546283\n",
      "Episode 59 return 92.0000003 discounted reward 37.3513933\n",
      "Episode 60 return 80.0000003 discounted reward 3.3734323\n",
      "Episode 61 return 91.0000003 discounted reward 32.6162543\n",
      "Episode 62 return 73.0000003 discounted reward -3.6035293\n",
      "Episode 63 return 78.0000003 discounted reward 0.8324803\n",
      "Episode 64 return 92.0000003 discounted reward 37.3513933\n",
      "Episode 65 return 88.0000003 discounted reward 21.0672493\n",
      "Episode 66 return 85.0000003 discounted reward 12.6480253\n",
      "Episode 67 return 87.0000003 discounted reward 17.9605243\n",
      "Episode 68 return 91.0000003 discounted reward 32.6162543\n",
      "Episode 69 return 87.0000003 discounted reward 17.9605243\n",
      "Episode 70 return 86.0000003 discounted reward 15.1644723\n",
      "Episode 71 return 86.0000003 discounted reward 15.1644723\n",
      "Episode 72 return 85.0000003 discounted reward 12.6480253\n",
      "Episode 73 return 61.0000003 discounted reward -8.1934483\n",
      "Episode 74 return 77.0000003 discounted reward -0.2507683\n",
      "Episode 75 return 88.0000003 discounted reward 21.0672493\n",
      "Episode 76 return 89.0000003 discounted reward 24.5191663\n",
      "Episode 77 return 93.0000003 discounted reward 42.6126593\n",
      "Episode 78 return 82.0000003 discounted reward 6.5104103\n",
      "Episode 79 return 80.0000003 discounted reward 3.3734323\n",
      "Episode 80 return 91.0000003 discounted reward 32.6162543\n",
      "Episode 81 return 94.0000003 discounted reward 48.4585103\n",
      "Episode 82 return 88.0000003 discounted reward 21.0672493\n",
      "Episode 83 return 93.0000003 discounted reward 42.6126593\n",
      "Episode 84 return 87.0000003 discounted reward 17.9605243\n",
      "Episode 85 return 83.0000003 discounted reward 8.3449003\n",
      "Episode 86 return 86.0000003 discounted reward 15.1644723\n",
      "Episode 87 return 91.0000003 discounted reward 32.6162543\n",
      "Episode 88 return 87.0000003 discounted reward 17.9605243\n",
      "Episode 89 return 90.0000003 discounted reward 28.3546283\n",
      "Episode 90 return 94.0000003 discounted reward 48.4585103\n",
      "Episode 91 return 95.0000003 discounted reward 54.9539003\n",
      "Episode 92 return 93.0000003 discounted reward 42.6126593\n",
      "Episode 93 return 93.0000003 discounted reward 42.6126593\n",
      "Episode 94 return 82.0000003 discounted reward 6.5104103\n",
      "Episode 95 return 84.0000003 discounted reward 10.3832223\n",
      "Episode 96 return 91.0000003 discounted reward 32.6162543\n",
      "Episode 97 return 88.0000003 discounted reward 21.0672493\n",
      "Episode 98 return 93.0000003 discounted reward 42.6126593\n",
      "Episode 99 return 84.0000003 discounted reward 10.3832223\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"GridWorld-v0\")\n",
    "file_name = \"log_files/GridWorld-q-learning/\" + str(time.time()) + \".csv\"\n",
    "with open(file_name, 'w+') as outfile:\n",
    "    writer = csv.writer(outfile, delimiter=\",\")\n",
    "    writer.writerow([\"r\", \"l\", \"t\"])\n",
    "\n",
    "def make_epsilon_greedy_policy(Q, epsilon, nA):\n",
    "    def policy_fn(observation):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        best_action = np.argmax(Q[observation])\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn\n",
    "\n",
    "def q_learning(env, num_episodes, discount_factor=0.9, alpha=0.2, epsilon=0.5, max_steps = 50):\n",
    "    Q = defaultdict(lambda: np.zeros(env.n_actions))\n",
    "    policy = make_epsilon_greedy_policy(Q, epsilon, env.n_actions)\n",
    "    env.reset()\n",
    "    env.render()\n",
    "    start = time.time()\n",
    "    for i_episode in range(num_episodes):\n",
    "        observation = env.reset()\n",
    "        discounted_reward = 0\n",
    "        total_reward = 0\n",
    "        for i in itertools.count():\n",
    "            a_prob = policy(observation)\n",
    "            a = np.random.choice([i for i in range(len(a_prob))], p = a_prob)\n",
    "            next_observation, reward, done, _ = env.step(a)\n",
    "            best_next_a = np.argmax(Q[next_observation])\n",
    "            Q[observation][a] += alpha * (reward + discount_factor * Q[next_observation][best_next_a] - Q[observation][a])\n",
    "            discounted_reward += (discount_factor**i)*reward\n",
    "            total_reward += reward\n",
    "            if done or i > max_steps:\n",
    "                print('Episode %d return %f3 discounted reward %f3' %(i_episode, total_reward, discounted_reward))\n",
    "                break\n",
    "            observation = next_observation\n",
    "        runtime = time.time() - start\n",
    "        with open(file_name, 'a') as outfile:\n",
    "            writer = csv.writer(outfile, delimiter=\",\")\n",
    "            writer.writerow((str(total_reward), str(i), str(runtime)))\n",
    "    return Q\n",
    "\n",
    "Q = q_learning(env, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human in the Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 return 49.0000003 discounted reward -9.4897763\n",
      "Episode 1 return 81.0000003 discounted reward 4.8593693\n",
      "Episode 2 return 85.0000003 discounted reward 12.6480253\n",
      "Episode 3 return 93.0000003 discounted reward 42.6126593\n",
      "Episode 4 return 94.0000003 discounted reward 48.4585103\n",
      "Episode 5 return 83.0000003 discounted reward 8.3449003\n",
      "Episode 6 return 82.0000003 discounted reward 6.5104103\n",
      "Episode 7 return 92.0000003 discounted reward 37.3513933\n",
      "Episode 8 return 82.0000003 discounted reward 6.5104103\n",
      "Episode 9 return 87.0000003 discounted reward 17.9605243\n",
      "Episode 10 return 89.0000003 discounted reward 24.5191663\n",
      "Episode 11 return 95.0000003 discounted reward 54.9539003\n",
      "Episode 12 return 86.0000003 discounted reward 15.1644723\n",
      "Episode 13 return 89.0000003 discounted reward 24.5191663\n",
      "Episode 14 return 81.0000003 discounted reward 4.8593693\n",
      "Episode 15 return 84.0000003 discounted reward 10.3832223\n",
      "Episode 16 return 87.0000003 discounted reward 17.9605243\n",
      "Episode 17 return 84.0000003 discounted reward 10.3832223\n",
      "Episode 18 return 95.0000003 discounted reward 54.9539003\n",
      "Episode 19 return 69.0000003 discounted reward -5.8032753\n",
      "Episode 20 return 95.0000003 discounted reward 54.9539003\n",
      "Episode 21 return 71.0000003 discounted reward -4.8188583\n",
      "Episode 22 return 93.0000003 discounted reward 42.6126593\n",
      "Episode 23 return 88.0000003 discounted reward 21.0672493\n",
      "Episode 24 return 88.0000003 discounted reward 21.0672493\n",
      "Episode 25 return 95.0000003 discounted reward 54.9539003\n",
      "Episode 26 return 94.0000003 discounted reward 48.4585103\n",
      "Episode 27 return 92.0000003 discounted reward 37.3513933\n",
      "Episode 28 return 86.0000003 discounted reward 15.1644723\n",
      "Episode 29 return 92.0000003 discounted reward 37.3513933\n",
      "Episode 30 return 84.0000003 discounted reward 10.3832223\n",
      "Episode 31 return 82.0000003 discounted reward 6.5104103\n",
      "Episode 32 return 88.0000003 discounted reward 21.0672493\n",
      "Episode 33 return 90.0000003 discounted reward 28.3546283\n",
      "Episode 34 return 92.0000003 discounted reward 37.3513933\n",
      "Episode 35 return 83.0000003 discounted reward 8.3449003\n",
      "Episode 36 return 77.0000003 discounted reward -0.2507683\n",
      "Episode 37 return 83.0000003 discounted reward 8.3449003\n",
      "Episode 38 return 89.0000003 discounted reward 24.5191663\n",
      "Episode 39 return 89.0000003 discounted reward 24.5191663\n",
      "Episode 40 return 81.0000003 discounted reward 4.8593693\n",
      "Episode 41 return 87.0000003 discounted reward 17.9605243\n",
      "Episode 42 return 95.0000003 discounted reward 54.9539003\n",
      "Episode 43 return -52.0000003 discounted reward -9.9582543\n",
      "Episode 44 return 91.0000003 discounted reward 32.6162543\n",
      "Episode 45 return 84.0000003 discounted reward 10.3832223\n",
      "Episode 46 return 79.0000003 discounted reward 2.0360893\n",
      "Episode 47 return 86.0000003 discounted reward 15.1644723\n",
      "Episode 48 return 91.0000003 discounted reward 32.6162543\n",
      "Episode 49 return 95.0000003 discounted reward 54.9539003\n",
      "Episode 50 return 65.0000003 discounted reward -7.2465293\n",
      "Episode 51 return 66.0000003 discounted reward -6.9405883\n",
      "Episode 52 return 79.0000003 discounted reward 2.0360893\n",
      "Episode 53 return 73.0000003 discounted reward -3.6035293\n",
      "Episode 54 return 95.0000003 discounted reward 54.9539003\n",
      "Episode 55 return 79.0000003 discounted reward 2.0360893\n",
      "Episode 56 return 91.0000003 discounted reward 32.6162543\n",
      "Episode 57 return 90.0000003 discounted reward 28.3546283\n",
      "Episode 58 return 87.0000003 discounted reward 17.9605243\n",
      "Episode 59 return 91.0000003 discounted reward 32.6162543\n",
      "Episode 60 return 89.0000003 discounted reward 24.5191663\n",
      "Episode 61 return 91.0000003 discounted reward 32.6162543\n",
      "Episode 62 return 81.0000003 discounted reward 4.8593693\n",
      "Episode 63 return 88.0000003 discounted reward 21.0672493\n",
      "Episode 64 return 89.0000003 discounted reward 24.5191663\n",
      "Episode 65 return 93.0000003 discounted reward 42.6126593\n",
      "Episode 66 return 90.0000003 discounted reward 28.3546283\n",
      "Episode 67 return 87.0000003 discounted reward 17.9605243\n",
      "Episode 68 return 95.0000003 discounted reward 54.9539003\n",
      "Episode 69 return 80.0000003 discounted reward 3.3734323\n",
      "Episode 70 return 86.0000003 discounted reward 15.1644723\n",
      "Episode 71 return 93.0000003 discounted reward 42.6126593\n",
      "Episode 72 return 77.0000003 discounted reward -0.2507683\n",
      "Episode 73 return 95.0000003 discounted reward 54.9539003\n",
      "Episode 74 return 92.0000003 discounted reward 37.3513933\n",
      "Episode 75 return 88.0000003 discounted reward 21.0672493\n",
      "Episode 76 return 77.0000003 discounted reward -0.2507683\n",
      "Episode 77 return 82.0000003 discounted reward 6.5104103\n",
      "Episode 78 return 82.0000003 discounted reward 6.5104103\n",
      "Episode 79 return 91.0000003 discounted reward 32.6162543\n",
      "Episode 80 return 87.0000003 discounted reward 17.9605243\n",
      "Episode 81 return 83.0000003 discounted reward 8.3449003\n",
      "Episode 82 return 90.0000003 discounted reward 28.3546283\n",
      "Episode 83 return 84.0000003 discounted reward 10.3832223\n",
      "Episode 84 return 85.0000003 discounted reward 12.6480253\n",
      "Episode 85 return 82.0000003 discounted reward 6.5104103\n",
      "Episode 86 return 93.0000003 discounted reward 42.6126593\n",
      "Episode 87 return 92.0000003 discounted reward 37.3513933\n",
      "Episode 88 return 95.0000003 discounted reward 54.9539003\n",
      "Episode 89 return 49.0000003 discounted reward -9.4897763\n",
      "Episode 90 return 91.0000003 discounted reward 32.6162543\n",
      "Episode 91 return 79.0000003 discounted reward 2.0360893\n",
      "Episode 92 return 75.0000003 discounted reward -2.1031223\n",
      "Episode 93 return 86.0000003 discounted reward 15.1644723\n",
      "Episode 94 return 68.0000003 discounted reward -6.2229483\n",
      "Episode 95 return 84.0000003 discounted reward 10.3832223\n",
      "Episode 96 return 90.0000003 discounted reward 28.3546283\n",
      "Episode 97 return 68.0000003 discounted reward -6.2229483\n",
      "Episode 98 return 83.0000003 discounted reward 8.3449003\n",
      "Episode 99 return 78.0000003 discounted reward 0.8324803\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"GridWorld-v0\")\n",
    "file_name = \"log_files/GridWorld-q-learning-human/\" + str(time.time()) + \".csv\"\n",
    "with open(file_name, 'w+') as outfile:\n",
    "    writer = csv.writer(outfile, delimiter=\",\")\n",
    "    writer.writerow([\"r\", \"l\", \"t\"])\n",
    "    \n",
    "def make_epsilon_greedy_policy(Q, epsilon, nA):\n",
    "    def policy_fn(observation):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        best_action = np.argmax(Q[observation])\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn\n",
    "\n",
    "def q_learning(env, num_episodes, discount_factor=0.9, alpha=0.2, epsilon=0.5, max_steps = 50):\n",
    "    Q = defaultdict(lambda: np.zeros(env.n_actions))\n",
    "    policy = make_epsilon_greedy_policy(Q, epsilon, env.n_actions)\n",
    "    env.reset()\n",
    "    env.render()\n",
    "    start = time.time()\n",
    "    for i_episode in range(num_episodes):\n",
    "        observation = env.reset()\n",
    "        discounted_reward = 0\n",
    "        total_reward = 0\n",
    "        for i in itertools.count():\n",
    "            a_prob = policy(observation)\n",
    "            a = np.random.choice([i for i in range(len(a_prob))], p = a_prob)\n",
    "            next_observation, reward, done, _ = env.step(a)\n",
    "            best_next_a = np.argmax(Q[next_observation])\n",
    "            Q[observation][a] += alpha * (reward + discount_factor * Q[next_observation][best_next_a] - Q[observation][a])\n",
    "            discounted_reward += (discount_factor**i)*reward\n",
    "            total_reward += reward\n",
    "            if done or i > max_steps:\n",
    "                print('Episode %d return %f3 discounted reward %f3' %(i_episode, total_reward, discounted_reward))\n",
    "                break\n",
    "            observation = next_observation\n",
    "        # human modifies the Q\n",
    "        # left red room\n",
    "        Q[0][2] += 1\n",
    "        Q[8][2] += 1\n",
    "        Q[16][2] += 1\n",
    "        Q[24][1] += 1\n",
    "        Q[32][0] += 1\n",
    "        Q[40][0] += 1\n",
    "        Q[48][0] += 1\n",
    "        \n",
    "        Q[1][2] += 1\n",
    "        Q[9][2] += 1\n",
    "        Q[17][2] += 1\n",
    "        Q[25][1] += 1\n",
    "        Q[33][0] += 1\n",
    "        Q[41][0] += 1\n",
    "        Q[49][0] += 1\n",
    "        \n",
    "        \n",
    "        # middle path \n",
    "        Q[26][1] += 1\n",
    "        \n",
    "        # middle blue room\n",
    "        Q[3][2] += 1\n",
    "        Q[11][2] += 1\n",
    "        Q[19][2] += 1\n",
    "        Q[27][1] += 1\n",
    "        Q[35][0] += 1\n",
    "        Q[43][0] += 1\n",
    "        Q[51][0] += 1\n",
    "        \n",
    "        Q[4][2] += 1\n",
    "        Q[12][2] += 1\n",
    "        Q[20][2] += 1\n",
    "        Q[28][1] += 1\n",
    "        Q[36][0] += 1\n",
    "        Q[44][0] += 1\n",
    "        Q[52][0] += 1\n",
    "        \n",
    "        # middle path \n",
    "        Q[29][1] += 1\n",
    "        \n",
    "        runtime = time.time() - start\n",
    "        with open(file_name, 'a') as outfile:\n",
    "            writer = csv.writer(outfile, delimiter=\",\")\n",
    "            writer.writerow((str(total_reward), str(i), str(runtime)))\n",
    "    return Q\n",
    "\n",
    "Q = q_learning(env, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
