{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import gym_gridworld\n",
    "import itertools\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 return -12.0000003 discounted reward -7.1757053\n",
      "Episode 1 return -12.0000003 discounted reward -7.1757053\n",
      "Episode 2 return -12.0000003 discounted reward -7.1757053\n",
      "Episode 3 return 95.0000003 discounted reward 54.9539003\n",
      "Episode 4 return -12.0000003 discounted reward -7.1757053\n",
      "Episode 5 return -12.0000003 discounted reward -7.1757053\n",
      "Episode 6 return -12.0000003 discounted reward -7.1757053\n",
      "Episode 7 return -12.0000003 discounted reward -7.1757053\n",
      "Episode 8 return -12.0000003 discounted reward -7.1757053\n",
      "Episode 9 return -12.0000003 discounted reward -7.1757053\n",
      "Episode 10 return 92.0000003 discounted reward 37.3513933\n",
      "Episode 11 return -12.0000003 discounted reward -7.1757053\n",
      "Episode 12 return 95.0000003 discounted reward 54.9539003\n",
      "Episode 13 return -12.0000003 discounted reward -7.1757053\n",
      "Episode 14 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 15 return -12.0000003 discounted reward -7.1757053\n",
      "Episode 16 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 17 return -12.0000003 discounted reward -7.1757053\n",
      "Episode 18 return 95.0000003 discounted reward 54.9539003\n",
      "Episode 19 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 20 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 21 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 22 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 23 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 24 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 25 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 26 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 27 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 28 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 29 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 30 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 31 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 32 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 33 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 34 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 35 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 36 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 37 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 38 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 39 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 40 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 41 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 42 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 43 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 44 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 45 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 46 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 47 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 48 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 49 return 97.0000003 discounted reward 70.1900003\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"GridWorld-v0\")\n",
    "\n",
    "def make_epsilon_greedy_policy(Q, epsilon, nA):\n",
    "    def policy_fn(observation):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        best_action = np.argmax(Q[observation])\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn\n",
    "\n",
    "def q_learning(env, num_episodes, discount_factor=0.9, alpha=0.1, epsilon=0, max_steps = 10):\n",
    "    Q = defaultdict(lambda: np.zeros(env.n_actions))\n",
    "    policy = make_epsilon_greedy_policy(Q, epsilon, env.n_actions)\n",
    "    env.reset()\n",
    "    env.render()\n",
    "    for i_episode in range(num_episodes):\n",
    "        observation = env.reset()\n",
    "        discounted_reward = 0\n",
    "        total_reward = 0\n",
    "        for i in itertools.count():\n",
    "            a_prob = policy(observation)\n",
    "            a = np.random.choice([i for i in range(len(a_prob))], p = a_prob)\n",
    "            next_observation, reward, done, _ = env.step(a)\n",
    "            best_next_a = np.argmax(Q[next_observation])\n",
    "            Q[observation][a] += alpha * (reward + discount_factor * Q[next_observation][best_next_a] - Q[observation][a])\n",
    "            discounted_reward += (discount_factor**i)*reward\n",
    "            total_reward += reward\n",
    "            if done or i > max_steps:\n",
    "                print('Episode %d return %f3 discounted reward %f3' %(i_episode, total_reward, discounted_reward))\n",
    "                break\n",
    "            observation = next_observation\n",
    "    return Q\n",
    "\n",
    "Q = q_learning(env, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human in the Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 return -12.0000003 discounted reward -7.1757053\n",
      "Episode 1 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 2 return 93.0000003 discounted reward 42.6126593\n",
      "Episode 3 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 4 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 5 return 96.0000003 discounted reward 62.1710003\n",
      "Episode 6 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 7 return 95.0000003 discounted reward 54.9539003\n",
      "Episode 8 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 9 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 10 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 11 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 12 return 96.0000003 discounted reward 62.1710003\n",
      "Episode 13 return 95.0000003 discounted reward 54.9539003\n",
      "Episode 14 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 15 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 16 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 17 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 18 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 19 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 20 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 21 return 95.0000003 discounted reward 54.9539003\n",
      "Episode 22 return 95.0000003 discounted reward 54.9539003\n",
      "Episode 23 return 95.0000003 discounted reward 54.9539003\n",
      "Episode 24 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 25 return 96.0000003 discounted reward 62.1710003\n",
      "Episode 26 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 27 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 28 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 29 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 30 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 31 return 94.0000003 discounted reward 48.4585103\n",
      "Episode 32 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 33 return 95.0000003 discounted reward 54.9539003\n",
      "Episode 34 return 95.0000003 discounted reward 54.9539003\n",
      "Episode 35 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 36 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 37 return 95.0000003 discounted reward 54.9539003\n",
      "Episode 38 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 39 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 40 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 41 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 42 return 95.0000003 discounted reward 54.9539003\n",
      "Episode 43 return 96.0000003 discounted reward 62.1710003\n",
      "Episode 44 return 95.0000003 discounted reward 54.9539003\n",
      "Episode 45 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 46 return 94.0000003 discounted reward 48.4585103\n",
      "Episode 47 return 97.0000003 discounted reward 70.1900003\n",
      "Episode 48 return 95.0000003 discounted reward 54.9539003\n",
      "Episode 49 return 97.0000003 discounted reward 70.1900003\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"GridWorld-v0\")\n",
    "\n",
    "def make_epsilon_greedy_policy(Q, epsilon, nA):\n",
    "    def policy_fn(observation):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        best_action = np.argmax(Q[observation])\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn\n",
    "\n",
    "def q_learning(env, num_episodes, discount_factor=0.9, alpha=0.5, epsilon=0.1, max_steps = 10):\n",
    "    Q = defaultdict(lambda: np.zeros(env.n_actions))\n",
    "    policy = make_epsilon_greedy_policy(Q, epsilon, env.n_actions)\n",
    "    env.reset()\n",
    "    env.render()\n",
    "    for i_episode in range(num_episodes):\n",
    "        observation = env.reset()\n",
    "        discounted_reward = 0\n",
    "        total_reward = 0\n",
    "        for i in itertools.count():\n",
    "            a_prob = policy(observation)\n",
    "            a = np.random.choice([i for i in range(len(a_prob))], p = a_prob)\n",
    "            next_observation, reward, done, _ = env.step(a)\n",
    "            best_next_a = np.argmax(Q[next_observation])\n",
    "            Q[observation][a] += alpha * (reward + discount_factor * Q[next_observation][best_next_a] - Q[observation][a])\n",
    "            discounted_reward += (discount_factor**i)*reward\n",
    "            total_reward += reward\n",
    "            if done or i > max_steps:\n",
    "                print('Episode %d return %f3 discounted reward %f3' %(i_episode, total_reward, discounted_reward))\n",
    "                break\n",
    "            observation = next_observation\n",
    "        # human modifies the Q\n",
    "        # left red room\n",
    "        Q[0][2] += 1\n",
    "        Q[5][2] += 1\n",
    "        Q[10][1] += 1\n",
    "        Q[15][0] += 1\n",
    "        Q[20][0] += 1\n",
    "        \n",
    "        Q[11][1] += 1\n",
    "        \n",
    "        # middle blue room\n",
    "        Q[2][2] += 1\n",
    "        Q[7][2] += 1\n",
    "        Q[12][1] += 1\n",
    "        Q[17][0] += 1\n",
    "        Q[23][0] += 1\n",
    "        \n",
    "        Q[13][1] += 1\n",
    "    return Q\n",
    "\n",
    "Q = q_learning(env, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
