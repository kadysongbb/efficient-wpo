{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "import csv\n",
    "import gym_gridworld\n",
    "import itertools\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 return -52.0000003 discounted reward -9.9582543\n",
      "Episode 1 return -52.0000003 discounted reward -9.9582543\n",
      "Episode 2 return -52.0000003 discounted reward -9.9582543\n",
      "Episode 3 return -52.0000003 discounted reward -9.9582543\n",
      "Episode 4 return -52.0000003 discounted reward -9.9582543\n",
      "Episode 5 return 51.0000003 discounted reward -9.3700943\n",
      "Episode 6 return -52.0000003 discounted reward -9.9582543\n",
      "Episode 7 return -52.0000003 discounted reward -9.9582543\n",
      "Episode 8 return -52.0000003 discounted reward -9.9582543\n",
      "Episode 9 return -52.0000003 discounted reward -9.9582543\n",
      "Episode 10 return -52.0000003 discounted reward -9.9582543\n",
      "Episode 11 return 86.0000003 discounted reward 15.1644723\n",
      "Episode 12 return 50.0000003 discounted reward -9.4330853\n",
      "Episode 13 return 75.0000003 discounted reward -2.1031223\n",
      "Episode 14 return 69.0000003 discounted reward -5.8032753\n",
      "Episode 15 return 64.0000003 discounted reward -7.5218763\n",
      "Episode 16 return 95.0000003 discounted reward 54.9539003\n",
      "Episode 17 return -52.0000003 discounted reward -9.9582543\n",
      "Episode 18 return 73.0000003 discounted reward -3.6035293\n",
      "Episode 19 return 93.0000003 discounted reward 42.6126593\n",
      "Episode 20 return 91.0000003 discounted reward 32.6162543\n",
      "Episode 21 return -52.0000003 discounted reward -9.9582543\n",
      "Episode 22 return 75.0000003 discounted reward -2.1031223\n",
      "Episode 23 return 86.0000003 discounted reward 15.1644723\n",
      "Episode 24 return 87.0000003 discounted reward 17.9605243\n",
      "Episode 25 return 72.0000003 discounted reward -4.2431763\n",
      "Episode 26 return 67.0000003 discounted reward -6.6006533\n",
      "Episode 27 return 93.0000003 discounted reward 42.6126593\n",
      "Episode 28 return 93.0000003 discounted reward 42.6126593\n",
      "Episode 29 return 51.0000003 discounted reward -9.3700943\n",
      "Episode 30 return 64.0000003 discounted reward -7.5218763\n",
      "Episode 31 return 92.0000003 discounted reward 37.3513933\n",
      "Episode 32 return 94.0000003 discounted reward 48.4585103\n",
      "Episode 33 return 87.0000003 discounted reward 17.9605243\n",
      "Episode 34 return 88.0000003 discounted reward 21.0672493\n",
      "Episode 35 return 94.0000003 discounted reward 48.4585103\n",
      "Episode 36 return 93.0000003 discounted reward 42.6126593\n",
      "Episode 37 return 86.0000003 discounted reward 15.1644723\n",
      "Episode 38 return 89.0000003 discounted reward 24.5191663\n",
      "Episode 39 return 92.0000003 discounted reward 37.3513933\n",
      "Episode 40 return 89.0000003 discounted reward 24.5191663\n",
      "Episode 41 return 92.0000003 discounted reward 37.3513933\n",
      "Episode 42 return 93.0000003 discounted reward 42.6126593\n",
      "Episode 43 return 91.0000003 discounted reward 32.6162543\n",
      "Episode 44 return 89.0000003 discounted reward 24.5191663\n",
      "Episode 45 return 83.0000003 discounted reward 8.3449003\n",
      "Episode 46 return 74.0000003 discounted reward -2.8928103\n",
      "Episode 47 return 95.0000003 discounted reward 54.9539003\n",
      "Episode 48 return 80.0000003 discounted reward 3.3734323\n",
      "Episode 49 return 91.0000003 discounted reward 32.6162543\n",
      "Episode 50 return 82.0000003 discounted reward 6.5104103\n",
      "Episode 51 return 82.0000003 discounted reward 6.5104103\n",
      "Episode 52 return 85.0000003 discounted reward 12.6480253\n",
      "Episode 53 return 85.0000003 discounted reward 12.6480253\n",
      "Episode 54 return 94.0000003 discounted reward 48.4585103\n",
      "Episode 55 return 77.0000003 discounted reward -0.2507683\n",
      "Episode 56 return 93.0000003 discounted reward 42.6126593\n",
      "Episode 57 return 82.0000003 discounted reward 6.5104103\n",
      "Episode 58 return 86.0000003 discounted reward 15.1644723\n",
      "Episode 59 return 93.0000003 discounted reward 42.6126593\n",
      "Episode 60 return 95.0000003 discounted reward 54.9539003\n",
      "Episode 61 return 86.0000003 discounted reward 15.1644723\n",
      "Episode 62 return 89.0000003 discounted reward 24.5191663\n",
      "Episode 63 return 83.0000003 discounted reward 8.3449003\n",
      "Episode 64 return 78.0000003 discounted reward 0.8324803\n",
      "Episode 65 return 92.0000003 discounted reward 37.3513933\n",
      "Episode 66 return 94.0000003 discounted reward 48.4585103\n",
      "Episode 67 return 93.0000003 discounted reward 42.6126593\n",
      "Episode 68 return 86.0000003 discounted reward 15.1644723\n",
      "Episode 69 return 74.0000003 discounted reward -2.8928103\n",
      "Episode 70 return 88.0000003 discounted reward 21.0672493\n",
      "Episode 71 return 87.0000003 discounted reward 17.9605243\n",
      "Episode 72 return 93.0000003 discounted reward 42.6126593\n",
      "Episode 73 return 89.0000003 discounted reward 24.5191663\n",
      "Episode 74 return 70.0000003 discounted reward -5.3369733\n",
      "Episode 75 return 93.0000003 discounted reward 42.6126593\n",
      "Episode 76 return 75.0000003 discounted reward -2.1031223\n",
      "Episode 77 return 89.0000003 discounted reward 24.5191663\n",
      "Episode 78 return 95.0000003 discounted reward 54.9539003\n",
      "Episode 79 return 90.0000003 discounted reward 28.3546283\n",
      "Episode 80 return 94.0000003 discounted reward 48.4585103\n",
      "Episode 81 return 63.0000003 discounted reward -7.7696883\n",
      "Episode 82 return 93.0000003 discounted reward 42.6126593\n",
      "Episode 83 return 93.0000003 discounted reward 42.6126593\n",
      "Episode 84 return 95.0000003 discounted reward 54.9539003\n",
      "Episode 85 return 90.0000003 discounted reward 28.3546283\n",
      "Episode 86 return 89.0000003 discounted reward 24.5191663\n",
      "Episode 87 return 93.0000003 discounted reward 42.6126593\n",
      "Episode 88 return 90.0000003 discounted reward 28.3546283\n",
      "Episode 89 return 78.0000003 discounted reward 0.8324803\n",
      "Episode 90 return 89.0000003 discounted reward 24.5191663\n",
      "Episode 91 return 80.0000003 discounted reward 3.3734323\n",
      "Episode 92 return 93.0000003 discounted reward 42.6126593\n",
      "Episode 93 return 86.0000003 discounted reward 15.1644723\n",
      "Episode 94 return 87.0000003 discounted reward 17.9605243\n",
      "Episode 95 return 93.0000003 discounted reward 42.6126593\n",
      "Episode 96 return 83.0000003 discounted reward 8.3449003\n",
      "Episode 97 return 89.0000003 discounted reward 24.5191663\n",
      "Episode 98 return 86.0000003 discounted reward 15.1644723\n",
      "Episode 99 return 92.0000003 discounted reward 37.3513933\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"GridWorld-v0\")\n",
    "file_name = \"log_files/GridWorld-q-learning/\" + str(time.time()) + \".csv\"\n",
    "with open(file_name, 'w+') as outfile:\n",
    "    writer = csv.writer(outfile, delimiter=\",\")\n",
    "    writer.writerow([\"r\", \"l\", \"t\"])\n",
    "\n",
    "def make_epsilon_greedy_policy(Q, epsilon, nA):\n",
    "    def policy_fn(observation):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        best_action = np.argmax(Q[observation])\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn\n",
    "\n",
    "def q_learning(env, num_episodes, discount_factor=0.9, alpha=0.2, epsilon=0.5, max_steps = 50):\n",
    "    Q = defaultdict(lambda: np.zeros(env.n_actions))\n",
    "    policy = make_epsilon_greedy_policy(Q, epsilon, env.n_actions)\n",
    "    env.reset()\n",
    "    env.render()\n",
    "    start = time.time()\n",
    "    for i_episode in range(num_episodes):\n",
    "        observation = env.reset()\n",
    "        discounted_reward = 0\n",
    "        total_reward = 0\n",
    "        for i in itertools.count():\n",
    "            a_prob = policy(observation)\n",
    "            a = np.random.choice([i for i in range(len(a_prob))], p = a_prob)\n",
    "            next_observation, reward, done, _ = env.step(a)\n",
    "            best_next_a = np.argmax(Q[next_observation])\n",
    "            Q[observation][a] += alpha * (reward + discount_factor * Q[next_observation][best_next_a] - Q[observation][a])\n",
    "            discounted_reward += (discount_factor**i)*reward\n",
    "            total_reward += reward\n",
    "            if done or i > max_steps:\n",
    "                print('Episode %d return %f3 discounted reward %f3' %(i_episode, total_reward, discounted_reward))\n",
    "                break\n",
    "            observation = next_observation\n",
    "        runtime = time.time() - start\n",
    "        with open(file_name, 'a') as outfile:\n",
    "            writer = csv.writer(outfile, delimiter=\",\")\n",
    "            writer.writerow((str(total_reward), str(i), str(runtime)))\n",
    "    return Q\n",
    "\n",
    "Q = q_learning(env, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human in the Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 return -52.0000003 discounted reward -9.9582543\n",
      "Episode 1 return 88.0000003 discounted reward 21.0672493\n",
      "Episode 2 return 85.0000003 discounted reward 12.6480253\n",
      "Episode 3 return 91.0000003 discounted reward 32.6162543\n",
      "Episode 4 return 87.0000003 discounted reward 17.9605243\n",
      "Episode 5 return 90.0000003 discounted reward 28.3546283\n",
      "Episode 6 return 91.0000003 discounted reward 32.6162543\n",
      "Episode 7 return 86.0000003 discounted reward 15.1644723\n",
      "Episode 8 return 91.0000003 discounted reward 32.6162543\n",
      "Episode 9 return 89.0000003 discounted reward 24.5191663\n",
      "Episode 10 return 94.0000003 discounted reward 48.4585103\n",
      "Episode 11 return 92.0000003 discounted reward 37.3513933\n",
      "Episode 12 return 83.0000003 discounted reward 8.3449003\n",
      "Episode 13 return 92.0000003 discounted reward 37.3513933\n",
      "Episode 14 return 86.0000003 discounted reward 15.1644723\n",
      "Episode 15 return 93.0000003 discounted reward 42.6126593\n",
      "Episode 16 return 91.0000003 discounted reward 32.6162543\n",
      "Episode 17 return 93.0000003 discounted reward 42.6126593\n",
      "Episode 18 return 87.0000003 discounted reward 17.9605243\n",
      "Episode 19 return 91.0000003 discounted reward 32.6162543\n",
      "Episode 20 return 95.0000003 discounted reward 54.9539003\n",
      "Episode 21 return 87.0000003 discounted reward 17.9605243\n",
      "Episode 22 return 81.0000003 discounted reward 4.8593693\n",
      "Episode 23 return 83.0000003 discounted reward 8.3449003\n",
      "Episode 24 return 92.0000003 discounted reward 37.3513933\n",
      "Episode 25 return 84.0000003 discounted reward 10.3832223\n",
      "Episode 26 return 94.0000003 discounted reward 48.4585103\n",
      "Episode 27 return 90.0000003 discounted reward 28.3546283\n",
      "Episode 28 return 91.0000003 discounted reward 32.6162543\n",
      "Episode 29 return 81.0000003 discounted reward 4.8593693\n",
      "Episode 30 return 89.0000003 discounted reward 24.5191663\n",
      "Episode 31 return 85.0000003 discounted reward 12.6480253\n",
      "Episode 32 return 92.0000003 discounted reward 37.3513933\n",
      "Episode 33 return 90.0000003 discounted reward 28.3546283\n",
      "Episode 34 return 64.0000003 discounted reward -7.5218763\n",
      "Episode 35 return 81.0000003 discounted reward 4.8593693\n",
      "Episode 36 return 89.0000003 discounted reward 24.5191663\n",
      "Episode 37 return 95.0000003 discounted reward 54.9539003\n",
      "Episode 38 return 71.0000003 discounted reward -4.8188583\n",
      "Episode 39 return 95.0000003 discounted reward 54.9539003\n",
      "Episode 40 return 91.0000003 discounted reward 32.6162543\n",
      "Episode 41 return 87.0000003 discounted reward 17.9605243\n",
      "Episode 42 return 88.0000003 discounted reward 21.0672493\n",
      "Episode 43 return 92.0000003 discounted reward 37.3513933\n",
      "Episode 44 return 88.0000003 discounted reward 21.0672493\n",
      "Episode 45 return 73.0000003 discounted reward -3.6035293\n",
      "Episode 46 return 95.0000003 discounted reward 54.9539003\n",
      "Episode 47 return 89.0000003 discounted reward 24.5191663\n",
      "Episode 48 return 91.0000003 discounted reward 32.6162543\n",
      "Episode 49 return 89.0000003 discounted reward 24.5191663\n",
      "Episode 50 return 90.0000003 discounted reward 28.3546283\n",
      "Episode 51 return 93.0000003 discounted reward 42.6126593\n",
      "Episode 52 return 75.0000003 discounted reward -2.1031223\n",
      "Episode 53 return 82.0000003 discounted reward 6.5104103\n",
      "Episode 54 return 79.0000003 discounted reward 2.0360893\n",
      "Episode 55 return 89.0000003 discounted reward 24.5191663\n",
      "Episode 56 return 90.0000003 discounted reward 28.3546283\n",
      "Episode 57 return 86.0000003 discounted reward 15.1644723\n",
      "Episode 58 return 81.0000003 discounted reward 4.8593693\n",
      "Episode 59 return 91.0000003 discounted reward 32.6162543\n",
      "Episode 60 return 86.0000003 discounted reward 15.1644723\n",
      "Episode 61 return 89.0000003 discounted reward 24.5191663\n",
      "Episode 62 return 88.0000003 discounted reward 21.0672493\n",
      "Episode 63 return 87.0000003 discounted reward 17.9605243\n",
      "Episode 64 return 94.0000003 discounted reward 48.4585103\n",
      "Episode 65 return 87.0000003 discounted reward 17.9605243\n",
      "Episode 66 return 77.0000003 discounted reward -0.2507683\n",
      "Episode 67 return 85.0000003 discounted reward 12.6480253\n",
      "Episode 68 return 94.0000003 discounted reward 48.4585103\n",
      "Episode 69 return 86.0000003 discounted reward 15.1644723\n",
      "Episode 70 return 82.0000003 discounted reward 6.5104103\n",
      "Episode 71 return 82.0000003 discounted reward 6.5104103\n",
      "Episode 72 return 89.0000003 discounted reward 24.5191663\n",
      "Episode 73 return 72.0000003 discounted reward -4.2431763\n",
      "Episode 74 return 75.0000003 discounted reward -2.1031223\n",
      "Episode 75 return 90.0000003 discounted reward 28.3546283\n",
      "Episode 76 return 72.0000003 discounted reward -4.2431763\n",
      "Episode 77 return 85.0000003 discounted reward 12.6480253\n",
      "Episode 78 return 68.0000003 discounted reward -6.2229483\n",
      "Episode 79 return 88.0000003 discounted reward 21.0672493\n",
      "Episode 80 return 85.0000003 discounted reward 12.6480253\n",
      "Episode 81 return 93.0000003 discounted reward 42.6126593\n",
      "Episode 82 return 89.0000003 discounted reward 24.5191663\n",
      "Episode 83 return 89.0000003 discounted reward 24.5191663\n",
      "Episode 84 return 84.0000003 discounted reward 10.3832223\n",
      "Episode 85 return 87.0000003 discounted reward 17.9605243\n",
      "Episode 86 return 80.0000003 discounted reward 3.3734323\n",
      "Episode 87 return 88.0000003 discounted reward 21.0672493\n",
      "Episode 88 return 74.0000003 discounted reward -2.8928103\n",
      "Episode 89 return 81.0000003 discounted reward 4.8593693\n",
      "Episode 90 return 74.0000003 discounted reward -2.8928103\n",
      "Episode 91 return 92.0000003 discounted reward 37.3513933\n",
      "Episode 92 return 84.0000003 discounted reward 10.3832223\n",
      "Episode 93 return 89.0000003 discounted reward 24.5191663\n",
      "Episode 94 return 87.0000003 discounted reward 17.9605243\n",
      "Episode 95 return 85.0000003 discounted reward 12.6480253\n",
      "Episode 96 return 93.0000003 discounted reward 42.6126593\n",
      "Episode 97 return 84.0000003 discounted reward 10.3832223\n",
      "Episode 98 return 92.0000003 discounted reward 37.3513933\n",
      "Episode 99 return 65.0000003 discounted reward -7.2465293\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"GridWorld-v0\")\n",
    "file_name = \"log_files/GridWorld-q-learning-human/\" + str(time.time()) + \".csv\"\n",
    "with open(file_name, 'w+') as outfile:\n",
    "    writer = csv.writer(outfile, delimiter=\",\")\n",
    "    writer.writerow([\"r\", \"l\", \"t\"])\n",
    "    \n",
    "def make_epsilon_greedy_policy(Q, epsilon, nA):\n",
    "    def policy_fn(observation):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        best_action = np.argmax(Q[observation])\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn\n",
    "\n",
    "def q_learning(env, num_episodes, discount_factor=0.9, alpha=0.2, epsilon=0.5, max_steps = 50):\n",
    "    Q = defaultdict(lambda: np.zeros(env.n_actions))\n",
    "    policy = make_epsilon_greedy_policy(Q, epsilon, env.n_actions)\n",
    "    env.reset()\n",
    "    env.render()\n",
    "    start = time.time()\n",
    "    for i_episode in range(num_episodes):\n",
    "        observation = env.reset()\n",
    "        discounted_reward = 0\n",
    "        total_reward = 0\n",
    "        for i in itertools.count():\n",
    "            a_prob = policy(observation)\n",
    "            a = np.random.choice([i for i in range(len(a_prob))], p = a_prob)\n",
    "            next_observation, reward, done, _ = env.step(a)\n",
    "            best_next_a = np.argmax(Q[next_observation])\n",
    "            Q[observation][a] += alpha * (reward + discount_factor * Q[next_observation][best_next_a] - Q[observation][a])\n",
    "            discounted_reward += (discount_factor**i)*reward\n",
    "            total_reward += reward\n",
    "            if done or i > max_steps:\n",
    "                print('Episode %d return %f3 discounted reward %f3' %(i_episode, total_reward, discounted_reward))\n",
    "                break\n",
    "            observation = next_observation\n",
    "        # human modifies the Q\n",
    "        # left red room\n",
    "        Q[0][2] += 1\n",
    "        Q[8][2] += 1\n",
    "        Q[16][2] += 1\n",
    "        Q[24][1] += 1\n",
    "        Q[32][0] += 1\n",
    "        Q[40][0] += 1\n",
    "        Q[48][0] += 1\n",
    "        \n",
    "        Q[1][2] += 1\n",
    "        Q[9][2] += 1\n",
    "        Q[17][2] += 1\n",
    "        Q[25][1] += 1\n",
    "        Q[33][0] += 1\n",
    "        Q[41][0] += 1\n",
    "        Q[49][0] += 1\n",
    "        \n",
    "        \n",
    "        # middle path \n",
    "        Q[26][1] += 1\n",
    "        \n",
    "        # middle blue room\n",
    "        Q[3][2] += 1\n",
    "        Q[11][2] += 1\n",
    "        Q[19][2] += 1\n",
    "        Q[27][1] += 1\n",
    "        Q[35][0] += 1\n",
    "        Q[43][0] += 1\n",
    "        Q[51][0] += 1\n",
    "        \n",
    "        Q[4][2] += 1\n",
    "        Q[12][2] += 1\n",
    "        Q[20][2] += 1\n",
    "        Q[28][1] += 1\n",
    "        Q[36][0] += 1\n",
    "        Q[44][0] += 1\n",
    "        Q[52][0] += 1\n",
    "        \n",
    "        # middle path \n",
    "        Q[29][1] += 1\n",
    "        \n",
    "        runtime = time.time() - start\n",
    "        with open(file_name, 'a') as outfile:\n",
    "            writer = csv.writer(outfile, delimiter=\",\")\n",
    "            writer.writerow((str(total_reward), str(i), str(runtime)))\n",
    "    return Q\n",
    "\n",
    "Q = q_learning(env, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
